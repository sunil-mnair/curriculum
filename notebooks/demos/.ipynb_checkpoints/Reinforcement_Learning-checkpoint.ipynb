{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7baeabab-2c8e-459e-85dd-cd9513a24b9c",
   "metadata": {},
   "source": [
    "# Re-Inforcement Learning\n",
    "\n",
    "## Summary:\n",
    "- This script demonstrates a simple Q-learning algorithm applied to a 5x5 grid world.\n",
    "- The agent starts in the top-left corner of the grid and aims to reach the bottom-right corner (goal).\n",
    "- Rewards are assigned for reaching the goal (+1) and penalties are given for every other move (-0.1).\n",
    "- The Q-table is updated over 1000 episodes to learn the optimal path from start to goal.\n",
    "- After training, the agent follows the learned policy to navigate the grid efficiently.\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd48e3a5-c7e6-4bd5-b20d-f3a33f90c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0dbc4a-ce84-4a7d-b4cb-abcb77eb57be",
   "metadata": {},
   "source": [
    "## Define the environment\n",
    "### (a simple 5x5 grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c3e17b-c0c4-401f-8f6b-50b822c047cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 5\n",
    "        self.state = (0, 0)  # Start in the top-left corner\n",
    "        self.goal = (4, 4)   # Goal is in the bottom-right corner\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to the starting state (top-left corner)\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Take an action and move to the next state\n",
    "        # Action can be: 0 (up), 1 (down), 2 (left), 3 (right)\n",
    "        row, col = self.state\n",
    "        \n",
    "        if action == 0:   # Move up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1: # Move down\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif action == 2: # Move left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 3: # Move right\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "        \n",
    "        # Update the current state\n",
    "        self.state = (row, col)\n",
    "        \n",
    "        # Define reward structure: +1 for reaching the goal, -0.1 for every other step\n",
    "        reward = 1 if self.state == self.goal else -0.1\n",
    "        done = self.state == self.goal\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "    def get_num_states(self):\n",
    "        # Return the total number of possible states in the grid\n",
    "        return self.grid_size * self.grid_size\n",
    "    \n",
    "    def get_num_actions(self):\n",
    "        # Return the number of possible actions (up, down, left, right)\n",
    "        return 4\n",
    "\n",
    "    def render(self):\n",
    "        # Draw the grid to show the agent's current position\n",
    "        grid = [['_' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        row, col = self.state\n",
    "        grid[row][col] = 'A'  # Mark the agent's position\n",
    "        grid[self.goal[0]][self.goal[1]] = 'G'  # Mark the goal position\n",
    "        \n",
    "        for line in grid:\n",
    "            print(' '.join(line))\n",
    "        print('\\n')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39252c5-dd22-4a21-8e03-cfae2391a83e",
   "metadata": {},
   "source": [
    "## Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "021c27b3-427f-42ce-9fc8-ef76e81ca59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97c294-5bb1-4fde-93dc-1c379af71d5f",
   "metadata": {},
   "source": [
    "## Re-Inforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce6d4fe6-a44f-4aea-ac03-335ca4a3299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 0: -9.999999999999977\n",
      "\n",
      "Episode: 100\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 100: 0.10000000000000009\n",
      "\n",
      "Episode: 200\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 200: 0.20000000000000007\n",
      "\n",
      "Episode: 300\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 300: 0.30000000000000004\n",
      "\n",
      "Episode: 400\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 400: 0.30000000000000004\n",
      "\n",
      "Episode: 500\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 500: 0.10000000000000009\n",
      "\n",
      "Episode: 600\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 600: 0.10000000000000009\n",
      "\n",
      "Episode: 700\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 700: 0.10000000000000009\n",
      "\n",
      "Episode: 800\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 800: 0.30000000000000004\n",
      "\n",
      "Episode: 900\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "Total Reward after Episode 900: 1.1102230246251565e-16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Q-learning\n",
    "alpha = 0.1          # Learning rate - how much we update the Q-values on each step\n",
    "gamma = 0.9          # Discount factor - how much we consider future rewards\n",
    "epsilon = 0.1        # Exploration rate - probability of choosing a random action (exploration)\n",
    "num_episodes = 1000  # Number of episodes - how many times we run through the environment\n",
    "\n",
    "# Initialize Q-table\n",
    "num_states = env.get_num_states()\n",
    "num_actions = env.get_num_actions()\n",
    "Q_table = np.zeros((num_states, num_actions))  # Q-table with states as rows and actions as columns\n",
    "\n",
    "def state_to_index(state, grid_size):\n",
    "    # Convert a (row, col) state into a single index for the Q-table\n",
    "    return state[0] * grid_size + state[1]\n",
    "\n",
    "# Q-learning loop - training the agent\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment for each episode\n",
    "    done = False\n",
    "    total_reward = 0  # Track total reward for the episode\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}\")\n",
    "        env.render()  # Render the grid to show the agent's current position\n",
    "    \n",
    "    while not done:\n",
    "        state_index = state_to_index(state, env.grid_size)\n",
    "        \n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        # With probability epsilon, choose a random action (exploration)\n",
    "        # Otherwise, choose the best known action (exploitation)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, num_actions - 1)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state_index])     # Exploit\n",
    "        \n",
    "        # Take action and observe the result\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state_index = state_to_index(next_state, env.grid_size)\n",
    "        total_reward += reward  # Accumulate reward for the episode\n",
    "        \n",
    "        # Update Q-value using the Q-learning update rule\n",
    "        # Q(s, a) = Q(s, a) + alpha * (reward + gamma * max(Q(s', a')) - Q(s, a))\n",
    "        best_next_action = np.max(Q_table[next_state_index])\n",
    "        Q_table[state_index, action] = Q_table[state_index, action] + alpha * (reward + gamma * best_next_action - Q_table[state_index, action])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "    \n",
    "    # Print the total reward for every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Total Reward after Episode {episode}: {total_reward}\\n\")\n",
    "\n",
    "# # Demonstration of the trained agent's performance\n",
    "# state = env.reset()  # Start from the initial state\n",
    "# done = False\n",
    "# print(\"\\nTrained agent's path:\")\n",
    "# while not done:\n",
    "#     env.render()  # Render the grid to show the agent's current position\n",
    "#     time.sleep(0.1)  # Add a small delay to visualize the path\n",
    "#     state_index = state_to_index(state, env.grid_size)\n",
    "#     # Choose the best action based on the trained Q-table\n",
    "#     action = np.argmax(Q_table[state_index])\n",
    "#     state, _, done = env.step(action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3faab-182f-4797-9909-70489fcbf83b",
   "metadata": {},
   "source": [
    "## Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d8c9898-f5d0-48e6-9586-9a32a24e93d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Episode 1:\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ A G\n",
      "\n",
      "\n",
      "\n",
      "Test Episode 2:\n",
      "A _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ A _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ _\n",
      "_ _ _ _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ A _ G\n",
      "\n",
      "\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ A G\n",
      "\n",
      "\n",
      "\n",
      "The trained agent successfully reached the goal in 2/2 test episodes.\n"
     ]
    }
   ],
   "source": [
    "# To test the performance of the trained agent, we will run a few episodes and check if the agent consistently reaches the goal\n",
    "num_test_episodes = 2\n",
    "successful_episodes = 0\n",
    "\n",
    "for test_episode in range(num_test_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    print(f\"\\nTest Episode {test_episode + 1}:\")\n",
    "    \n",
    "    while not done and steps < 50:  # Limit the number of steps to avoid infinite loops\n",
    "        env.render()  # Render the grid to show the agent's current position\n",
    "        time.sleep(0.1)  # Add a small delay to visualize the path\n",
    "        state_index = state_to_index(state, env.grid_size)\n",
    "        action = np.argmax(Q_table[state_index])  # Use the trained policy\n",
    "        state, _, done = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "    if done:\n",
    "        successful_episodes += 1\n",
    "\n",
    "print(f\"\\nThe trained agent successfully reached the goal in {successful_episodes}/{num_test_episodes} test episodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cab67d-fd73-4ff8-b0c1-7149ebeaaac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
